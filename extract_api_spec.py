# pdf2api_excel_auto_index.py
import re, json, shutil
from pathlib import Path
from typing import List, Dict
import pandas as pd
import pdfplumber

# ===== è·¯å¾‘è¨­å®š =====
ROOT = Path(__file__).resolve().parent
SPEC_DIR = (ROOT / "spec_input").resolve()        # æ”¾ PDF çš„è³‡æ–™å¤¾
OUTPUT_DIR = (ROOT / "output").resolve()          # è¼¸å‡ºè³‡æ–™å¤¾
OUTPUT_XLSX = OUTPUT_DIR / "API_upload_.xlsx"     # æœ€çµ‚è¼¸å‡ºæª”å


# ===== ç›®éŒ„ç¢ºä¿å­˜åœ¨ =====
def ensure_dirs():
    """è‹¥ç¼ºå°‘ spec_input / output å°±è‡ªå‹•å»ºç«‹"""
    SPEC_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# ===== æ¸…ç©º output =====
def clean_output(dir_: Path):
    """
    å»ºç«‹ä¸¦æ¸…ç©º output ç›®éŒ„ï¼ˆåˆªé™¤èˆŠæª”/èˆŠè³‡æ–™å¤¾ï¼‰ï¼Œ
    ä½†ä¿ç•™ä»¥ . é–‹é ­çš„æª”æ¡ˆï¼ˆä¾‹å¦‚ .gitkeepã€.gitignoreï¼‰
    """
    dir_.mkdir(parents=True, exist_ok=True)
    for p in dir_.iterdir():
        # è·³é .gitkeepã€.gitignore ç­‰éš±è—æª”
        if p.name.startswith("."):
            continue

        if p.is_file():
            try:
                p.unlink()
            except Exception:
                pass
        elif p.is_dir():
            shutil.rmtree(p, ignore_errors=True)


# ===== è®€å– PDF æ–‡å­— =====
def read_pdf_text(pdf: Path) -> str:
    """æŠŠæ•´ä»½ PDF çš„å¯é¸æ–‡å­—ä¸²èµ·ä¾†ï¼›åšç°¡å–®å…¨å½¢â†’åŠå½¢çš„æ¨™æº–åŒ–"""
    lines: List[str] = []
    with pdfplumber.open(pdf) as doc:
        for page in doc.pages:
            t = page.extract_text() or ""
            if t:
                t = (
                    t.replace("ï¼š", ":")
                    .replace("ï¼ˆ", "(")
                    .replace("ï¼‰", ")")
                    .replace("ï¼", ".")
                    .replace("ï¼", "-")
                )
                lines.append(t.strip())
    return "\n".join(lines)


# ===== æ‰¾å‡ºæ‰€æœ‰ API URL =====
def find_urls(text: str) -> List[str]:
    """å¾å…¨æ–‡æ‰¾å‡ºæ‰€æœ‰ 'POST /...' çš„ URLï¼ˆä¾å‡ºç¾é †åºå»é‡ï¼‰"""
    urls, seen = [], set()
    for m in re.finditer(r"POST\s+(/\S+)", text):
        u = m.group(1)
        if u not in seen:
            seen.add(u)
            urls.append(u)
    return urls


# ===== æŠ“ URL é™„è¿‘çš„å¤šæ®µ JSON =====
def json_blocks_near(text: str, center: int, radius=30000, max_blocks=12) -> List[str]:
    """ä»¥ URL ä½ç½®ç‚ºä¸­å¿ƒï¼Œå¾€å‰å¾Œæƒæ–‡å­—ï¼Œæ“·å–å¤šæ®µã€å¹³è¡¡å¤§æ‹¬è™Ÿã€çš„ JSON"""
    s = max(0, center - radius)
    e = min(len(text), center + radius)
    span = text[s:e]
    out, i = [], 0
    while len(out) < max_blocks:
        p = span.find("{", i)
        if p == -1:
            break
        depth, start = 0, p
        for j in range(p, len(span)):
            ch = span[j]
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    raw = span[start : j + 1].strip()
                    if len(raw) > 50:
                        out.append(raw)
                    i = j + 1
                    break
        else:
            break
    return out


# ===== å¾å¤šæ®µ JSON ä¸­æŒ‘å‡º Input/Output =====
def pick_io(blocks: List[str]) -> (str, str):
    def is_resp(b):
        t = b.lower()
        return any(k in t for k in ["msgrshdr", "rspcode", "responsejson", "error"])

    def is_req(b):
        t = b.lower()
        return any(k in t for k in ["securitycontext", "custid", "userid", "data", "body"]) and not is_resp(b)

    inp = next((b for b in blocks if is_req(b)), "")
    out = next((b for b in blocks if is_resp(b)), "")

    if not inp and blocks:
        inp = max(blocks, key=len)
    if not out:
        rest = [b for b in blocks if b is not inp]
        out = max(rest, key=len) if rest else ""

    def norm(s):
        try:
            return json.dumps(json.loads(s), ensure_ascii=False, indent=2)
        except Exception:
            return s

    return norm(inp), norm(out)


# ===== ä¸»ç¨‹å¼ =====
def main():
    ensure_dirs()           # ğŸ‘ˆ æ–°å¢ï¼šå…ˆç¢ºä¿å…©å€‹è³‡æ–™å¤¾å­˜åœ¨
    clean_output(OUTPUT_DIR)

    rows: List[Dict[str, str]] = []
    idx = 1  # å…¨åŸŸç´¯åŠ  Index

    for pdf in sorted(SPEC_DIR.glob("*.pdf")):
        text = read_pdf_text(pdf)
        if not text.strip():
            rows.append(
                {
                    "Index": idx,
                    "FileName": pdf.name,
                    "URL": "",
                    "Method": "POST",
                    "Inputï¼ˆä¸Šè¡Œ JSONï¼‰": "",
                    "Response Code": "200",
                    "Outputï¼ˆä¸‹è¡Œ JSONï¼‰": "",
                }
            )
            idx += 1
            continue

        urls = find_urls(text)
        for url in urls:
            m = re.search(re.escape(url), text)
            blocks = json_blocks_near(text, m.start()) if m else []
            inp, out = pick_io(blocks)
            rows.append(
                {
                    "Index": idx,
                    "FileName": pdf.name,
                    "URL": url,
                    "Method": "POST",
                    "Inputï¼ˆä¸Šè¡Œ JSONï¼‰": inp,
                    "Response Code": "200",
                    "Outputï¼ˆä¸‹è¡Œ JSONï¼‰": out,
                }
            )
            idx += 1

    # å­˜æˆ Excel
    df = pd.DataFrame(
        rows,
        columns=[
            "Index",
            "FileName",
            "URL",
            "Method",
            "Inputï¼ˆä¸Šè¡Œ JSONï¼‰",
            "Response Code",
            "Outputï¼ˆä¸‹è¡Œ JSONï¼‰",
        ],
    )
    with pd.ExcelWriter(OUTPUT_XLSX, engine="xlsxwriter") as w:
        df.to_excel(w, index=False, sheet_name="API")
        ws = w.sheets["API"]
        ws.set_column(0, 0, 8)   # Index
        ws.set_column(1, 1, 32)  # FileName
        ws.set_column(2, 2, 46)  # URL
        ws.set_column(3, 3, 10)  # Method
        ws.set_column(4, 4, 90)  # Input
        ws.set_column(5, 5, 12)  # Response Code
        ws.set_column(6, 6, 90)  # Output

    print(f"âœ… å®Œæˆï¼š{OUTPUT_XLSX}")

if __name__ == "__main__":
    main()